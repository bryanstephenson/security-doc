<?xml version="1.0" encoding="UTF-8"?>
<section xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink"
  version="5.0"
  xml:id="security-services-for-instances">
  <?dbhtml stop-chunking?>
    <title>Security services for instances</title>
    <para>One of the virtues of running instances in a virtualized environment is that it opens up new opportunities for security controls that are not typically available when deploying onto bare metal. There are several technologies that can be applied to the virtualization stack that bring improved information assurance for cloud tenants.</para>
    <para>Deployers or users of OpenStack with strong security requirements may want to consider deploying these technologies. Not all are applicable in every situation, indeed in some cases technologies may be ruled out for use in a cloud because of prescriptive business requirements. Similarly some technologies inspect instance data such as run state which may be undesirable to the users of the system.</para>
    <para>In this chapter we explore these technologies and describe the situations where they can be used to enhance security for instances or underlying instances. We also seek to highlight where privacy concerns may exist. These include data pass through, introspection, or providing a source of entropy. In this section we highlight the following additional security services:</para>
    <itemizedlist><listitem>
        <para>Entropy to instances</para>
      </listitem>
      <listitem>
        <para>Scheduling instances to nodes</para>
      </listitem>
      <listitem>
        <para>Trusted images</para>
      </listitem>
      <listitem>
        <para>Instance migrations</para>
      </listitem>
</itemizedlist>
    <section xml:id="security-services-for-instances-idp122544">
      <title>Entropy to instances</title>
      <para>We consider entropy to refer to the quality and source of random data that is available to an instance. Cryptographic technologies typically rely heavily on randomness, requiring a high quality pool of entropy to draw from. It is typically hard for a virtual machine to get enough entropy to support these operations. Entropy starvation can manifest in instances as something seemingly unrelated for example, slow boot times because the instance is waiting for ssh key generation. Entropy starvation may also motivate users to employ poor quality entropy sources from within the instance, making applications running in the cloud less secure overall.</para>
      <para>Fortunately, a cloud architect may address these issues by providing a high quality source of entropy to the cloud instances. This can be done by having enough hardware random number generators (HRNG) in the cloud to support the instances. In this case, "enough" is somewhat domain specific. For everyday operations, a modern HRNG is likely to produce enough entropy to support 50-100 compute nodes. High bandwidth HRNGs, such as the RdRand instruction available with Intel Ivy Bridge and newer processors could potentially handle more nodes. For a given cloud, an architect needs to understand the application requirements to ensure that sufficient entropy is available.</para>
      <para>Once the entropy is available in the cloud, the next step is getting that entropy into the instances. Tools such as the entropy gathering daemon (<link xlink:href="http://egd.sourceforge.net/">EGD</link>) provide a way to fairly and securely distribute entropy through a distributed system. Support exists for using the EGD as an entropy source for LibVirt.</para>
      <para>Compute support for these features is not generally available, but it would only require a moderate amount of work for implementors to integrate this functionality.</para>
    </section>
    <section xml:id="security-services-for-instances-idp128240">
      <title>Scheduling instances to nodes</title>
      <para>Before an instance is created, a host for the image instantiation must be selected. This selection is performed by the <systemitem class="service">nova-scheduler</systemitem> which determines how to dispatch compute and volume requests.</para>
      <para>The filter scheduler is the default scheduler for OpenStack Compute,
      although other schedulers exist (see the section <link
        xlink:href="http://docs.openstack.org/trunk/config-reference/content/section_compute-scheduler.html"
        >Scheduling</link> in the <citetitle>OpenStack Configuration
        Reference</citetitle>). The filter scheduler works in collaboration with
      'filters' to decide where an instance should be started. This process of
      host selection allows administrators to fulfill many different security
      requirements. Depending on the cloud deployment type for example, one
      could choose to have tenant instances reside on the same hosts whenever
      possible if data isolation was a primary concern, conversely one could
      attempt to have instances for a tenant reside on as many different hosts
      as possible for availability or fault tolerance reasons. The following
      diagram demonstrates how the filter scheduler works:</para>
      <para><inlinemediaobject><imageobject role="html">
            <imagedata contentdepth="400" contentwidth="550" fileref="static/filteringWorkflow1.png" format="PNG" scalefit="1"/>
          </imageobject>
<imageobject role="fo">
            <imagedata contentdepth="100%" fileref="static/filteringWorkflow1.png" format="PNG" scalefit="1" width="100%"/>
          </imageobject>
</inlinemediaobject></para>
      <para>The use of scheduler filters may be used to segregate customers, data, or even discard machines of the cloud that cannot be attested as secure. This generally applies to all OpenStack projects offering a scheduler. When building a cloud, you may choose to implement scheduling filters for a variety of security-related purposes.</para>
      <para>Below we highlight a few of the filters that may be useful in a security context, depending on your requirements, the full set of filter documentation is documented in the <link xlink:href="http://docs.openstack.org/trunk/config-reference/content/filter-scheduler.html">Filter Scheduler</link> section of the <citetitle>OpenStack Configuration Reference</citetitle>.</para>
      <para><emphasis role="bold">Tenant Driven Whole Host Reservation</emphasis></para>
      <para>There currently exists a <link xlink:href="https://blueprints.launchpad.net/nova/+spec/whole-host-allocation">blueprint for whole host reservation</link> -  This would allow a tenant to exclusively reserve hosts for only it's instances, incurring extra costs.</para>
      <section xml:id="security-services-for-instances-idp140080">
        <title>Host aggregates</title>
        <para>While not a filter in themselves, host aggregates allow
        administrators to assign key-value pairs to groups of
        machines. This allows cloud administrators, not users, to
        partition up their compute host resources. Each node can have
        multiple aggregates (see the <link
        xlink:href="http://docs.openstack.org/trunk/config-reference/content/host-aggregates.html">Host
        aggregates</link> section of the <citetitle>OpenStack
        Configuration Reference</citetitle> for more information on
        creating and managing aggregates).</para>
      </section>
      <section xml:id="security-services-for-instances-idp142048">
        <title>AggregateMultiTenancyIsolation</title>
        <para>Isolates tenants to specific host aggregates. If a host is in an aggregate that has the metadata key <literal>filter_tenant_id</literal> it will only create instances from that tenant (or list of tenants). A host can be in multiple aggregates. If a host does not belong to an aggregate with the metadata key, it can create instances from all tenants.</para>
      </section>
      <section xml:id="security-services-for-instances-idp143968">
        <title>DifferentHostFilter</title>
        <para>Schedule the instance on a different host from a set of instances. To take advantage of this filter, the requester must pass a scheduler hint, using <literal>different_host</literal> as the key and a list of instance uuids as the value. This filter is the opposite of the <literal>SameHostFilter</literal>.</para>
      </section>
      <section xml:id="security-services-for-instances-idp146208">
        <title>GroupAntiAffinityFilter</title>
        <para>The GroupAntiAffinityFilter ensures that each instance in a group is on a different host. To take advantage of this filter, the requester must pass a scheduler hint, using <literal>group</literal> as the key and a list of instance uuids as the value.</para>
      </section>
      <section xml:id="security-services-for-instances-idp148000">
        <title>Trusted compute pools</title>
        <para>There exists a scheduler filter which integrates with the <link xlink:href="https://github.com/OpenAttestation/OpenAttestation">Open Attestation Project</link> (OATS) to define scheduler behavior according to the attestation of PCRs received from a system using Intel TXT.</para>
        <para>It is unclear if this feature is compatible with AMD's similar SEM, although the OpenAttestation agent relies on the vendor-agnostic <link xlink:href="http://trousers.sourceforge.net/">TrouSerS library</link>.</para>
      </section>
    </section>
    <section xml:id="security-services-for-instances-idp150416">
      <title>Trusted images</title>
      <para>With regards to images, users will be working with pre-installed images or images that they upload themselves. In both cases, users will want to ensure that the image they are ultimately running has not been tampered with. This requires some source of truth such as a checksum for the known good version of an image as well as verification of the running image. This section describes the current best practices around image handling, while also calling out some of the existing gaps in this space.</para>
      <section xml:id="security-services-for-instances-idp151952">
        <title>Image creation process</title>
        <para>The OpenStack Documentation provides guidance on how to
        create and upload an image to the Image Service. Additionally
        it is assumed that you have a process by which you install and
        harden operating systems. Thus, the following items will
        provide additional guidance on how to ensure your images are
        transferred securely into OpenStack. There are a variety of
        options for obtaining images. Each has specific steps that
        help validate the image's provenance.</para>
        <para>The first option is to obtain boot media from a trusted source.</para>
        <screen><prompt>$</prompt> <userinput>mkdir -p /tmp/download_directorycd /tmp/download_directory</userinput>
<prompt>$</prompt> <userinput>wget http://mirror.anl.gov/pub/ubuntu-iso/CDs/precise/ubuntu-12.04.2-server-amd64.iso</userinput>
<prompt>$</prompt> <userinput>wget http://mirror.anl.gov/pub/ubuntu-iso/CDs/precise/SHA256SUMS</userinput>
<prompt>$</prompt> <userinput>wget http://mirror.anl.gov/pub/ubuntu-iso/CDs/precise/SHA256SUMS.gpg</userinput>
<prompt>$</prompt> <userinput>gpg --keyserver hkp://keyserver.ubuntu.com --recv-keys 0xFBB75451</userinput>
<prompt>$</prompt> <userinput>gpg --verify SHA256SUMS.gpg SHA256SUMSsha256sum -c SHA256SUMS 2&gt;&amp;1 | grep OK</userinput></screen>
        <para>The second option is to use the <link xlink:href="http://docs.openstack.org/image-guide/content/"><citetitle>OpenStack Virtual Machine Image Guide</citetitle></link>. In this case, you will want to follow your organizations OS hardening guidelines or those provided by a trusted third-party such as the <link xlink:href="http://iase.disa.mil/stigs/os/unix/red_hat.html">RHEL6 STIG</link>.</para>
        <para>The final option is to use an automated image builder. The following example uses the Oz image builder. The OpenStack community has recently created a newer tool worth investigating: disk-image-builder. We have not evaluated this tool from a security perspective.</para>
        <para>Example of RHEL 6 CCE-26976-1 which will help implement NIST 800-53 Section<emphasis>AC-19(d) in</emphasis> Oz.</para>
        <programlisting>&lt;template&gt;
&lt;name&gt;centos64&lt;/name&gt;
&lt;os&gt;
  &lt;name&gt;RHEL-6&lt;/name&gt;
  &lt;version&gt;4&lt;/version&gt;
  &lt;arch&gt;x86_64&lt;/arch&gt;
  &lt;install type='iso'&gt;
  &lt;iso&gt;http://trusted_local_iso_mirror/isos/x86_64/RHEL-6.4-x86_64-bin-DVD1.iso&lt;/iso&gt;
  &lt;/install&gt;
  &lt;rootpw&gt;CHANGE THIS TO YOUR ROOT PASSWORD&lt;/rootpw&gt;
&lt;/os&gt;
&lt;description&gt;RHEL 6.4 x86_64&lt;/description&gt;
&lt;repositories&gt;
  &lt;repository name='epel-6'&gt;
    &lt;url&gt;http://download.fedoraproject.org/pub/epel/6/$basearch&lt;/url&gt;
    &lt;signed&gt;no&lt;/signed&gt;
  &lt;/repository&gt;
&lt;/repositories&gt;
&lt;packages&gt;
  &lt;package name='epel-release'/&gt;
  &lt;package name='cloud-utils'/&gt;
  &lt;package name='cloud-init'/&gt;
&lt;/packages&gt;
&lt;commands&gt;
  &lt;command name='update'&gt;
  yum update
  yum clean all
  sed -i '/^HWADDR/d' /etc/sysconfig/network-scripts/ifcfg-eth0
  echo -n &gt; /etc/udev/rules.d/70-persistent-net.rules
  echo -n &gt; /lib/udev/rules.d/75-persistent-net-generator.rules
  chkconfig --level 0123456 autofs off
  service autofs stop
  &lt;/command&gt;
&lt;/commands&gt;
&lt;/template&gt;</programlisting>
        <para>It is recommended to avoid the manual image building process as it is complex and prone to error. Additionally, using an automated system like Oz for image building or a configuration management utility like Chef or Puppet for post-boot image hardening gives you the ability to produce a consistent image as well as track compliance of your base image to its respective hardening guidelines over time.</para>
        <para>If subscribing to a public cloud service, you should check with the cloud provider for an outline of the process used to produce their default images. If the provider allows you to upload your own images, you will want to ensure that you are able to verify that your image was not modified before using it to create an instance. To do this, refer to the following section on Image Provenance.</para>
      </section>
      <section xml:id="security-services-for-instances-idp166272">
        <title>Image provenance and validation</title>
        <para>Unfortunately, it is not currently possible to force Compute to validate an image hash immediately prior to starting an instance. To understand the situation, we begin with a brief overview of how images are handled around the time of image launch.</para>
        <para>Images come from the glance service to the nova service on a node. This transfer should be protected by running over SSL. Once the image is on the node, it is verified with a basic checksum and then it's disk is expanded based on the size of the instance being launched. If, at a later time, the same image is launched with the same instance size on this node, it will be launched from the same expanded image. Since this expanded image is not re-verified before launching, it could be tampered with and the user would not have any way of knowing, beyond a manual inspection of the files in the resulting image.</para>
        <para>We hope that future versions of Compute and/or the Image Service will offer support for validating the image hash before each instance launch. An alternative option that would be even more powerful would be allow users to sign an image and then have the signature validated when the instance is launched.</para>
      </section>
    </section>
    <section xml:id="security-services-for-instances-idp170576">
      <title>Instance migrations</title>
      <para>
      OpenStack and the underlying virtualization layers provide for
      the live migration of images between OpenStack nodes allowing
      you to seamlessly perform rolling upgrades of your OpenStack
      compute nodes without instance downtime. However, live
      migrations also come with their fair share of risk. To
      understand the risks involved, it is important to first
      understand how a live migration works. The following are the
      high level steps preformed during a live migration.
      </para>
      <orderedlist>
          <listitem><para>Start instance on destination host</para> </listitem>
          <listitem><para>Transfer memory</para> </listitem>
          <listitem><para>Stop the guest &amp; sync disks</para> </listitem>
          <listitem><para>Transfer state</para> </listitem>
          <listitem><para>Start the guest</para> </listitem>
      </orderedlist>
      <section xml:id="security-services-for-instances-idp177552">
        <title>Live migration risks</title>
        <para>At various stages of the live migration process the contents of an instances run time memory and disk are transmitted over the network in plain text. Thus there are several risks that need to be addressed when using live migration. The following in-exhaustive list details some of these risks:</para>
        <itemizedlist><listitem>
            <para><emphasis>Denial of Service (DoS)</emphasis>: If something fails during the migration process, the instance could be lost.</para>
          </listitem>
          <listitem>
            <para><emphasis>Data exposure</emphasis>: Memory or disk transfers must be handled securely.</para>
          </listitem>
          <listitem>
            <para><emphasis>Data manipulation</emphasis>: If memory or disk transfers are not handled securely, then an attacker could manipulate user data during the migration.</para>
          </listitem>
          <listitem>
            <para><emphasis>Code injection</emphasis>: If memory or disk transfers are not handled securely, then an attacker could manipulate executables, either on disk or in memory, during the migration.</para>
          </listitem>
        </itemizedlist>
      </section>
      <section xml:id="security-services-for-instances-idp183744">
        <title>Live migration mitigations</title>
        <para>There are several methods to mitigate some of the risk associated with live migrations, the following list details some of these:</para>
        <itemizedlist><listitem>
            <para>Disable live migration</para>
          </listitem>
          <listitem>
            <para>Isolated migration network</para>
          </listitem>
          <listitem>
            <para>Encrypted live migration</para>
          </listitem>
        </itemizedlist>
        <section xml:id="security-services-for-instances-idp187568">
          <title>Disable live migration</title>
          <para>At this time, live migration is enabled in OpenStack
          by default. Live migrations can be disabled by adding the
          following lines to the nova <filename>policy.json</filename>
          file:</para>
          <programlisting>"compute_extension:admin_actions:migrate": "!",
"compute_extension:admin_actions:migrateLive": "!",</programlisting>
        </section>
        <section xml:id="security-services-for-instances-idp189488">
          <title>Migration network</title>
          <para>As a general practice, live migration traffic should be restricted to the management security domain, see <xref linkend="security-boundaries-and-threats-idp55632" />. With live migration traffic, due to its plain text nature and the fact that you are transferring the contents of disk and memory of a running instance, it is recommended you further separate live migration traffic onto a dedicated network. Isolating the traffic to a dedicated network can reduce the risk of exposure.</para>
        </section>
        <section xml:id="security-services-for-instances-idp191072">
          <title>Encrypted live migration</title>
          <para>If there is a sufficient business case for keeping live migration enabled, then libvirtd can provide encrypted tunnels for the live migrations. However, this feature is not currently exposed in either the OpenStack Dashboard or nova-client commands, and can only be accessed through manual configuration of libvirtd. The live migration process then changes to the following high-level steps:</para>
<orderedlist>
  <listitem><para>Instance data is copied from the hypervisor to libvirtd.</para></listitem>
  <listitem><para>An encrypted tunnel is created between libvirtd processes on both source and destination hosts.</para></listitem>
  <listitem><para>Destination libvirtd host copies the instances back to an underlying hypervisor.</para></listitem>
</orderedlist>
        </section>
      </section>
    </section>
</section>
